{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditi-bairagi61/Ransomware-Attack-Record/blob/main/random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm8byD9iN2qs",
        "outputId": "16fd3087-08e3-41e0-dc04-a63f212ccf22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (Benign): 0.8932038834951457\n",
            "F1 Score (Benign): 0.9435897435897436\n",
            "Recall (Benign): 0.8932038834951457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Used Random forest Algorithm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "# Your code for calculating evaluation metrics\n",
        "\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(\"/content/Dataset.txt\", header=None)\n",
        "\n",
        "# Split the dataset into features (x) and target (y)\n",
        "x = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "sss\n",
        "# Resample the dataset to handle class imbalance\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "x_resampled, y_resampled = oversampler.fit_resample(x, y)\n",
        "\n",
        "# Split the resampled data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=75)\n",
        "\n",
        "# Instantiate and train the RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=50, random_state=10)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Filter predictions and true labels for 'Benign' class\n",
        "benign_indices = (y_test == 'Benign')\n",
        "y_test_benign = y_test[benign_indices]\n",
        "y_pred_benign = y_pred[benign_indices]\n",
        "\n",
        "# Calculate evaluation metrics for 'Benign' class\n",
        "accuracy_benign = accuracy_score(y_test_benign, y_pred_benign)\n",
        "f1_benign = f1_score(y_test_benign, y_pred_benign, average='weighted')\n",
        "recall_benign = recall_score(y_test_benign, y_pred_benign, average='weighted')\n",
        "\n",
        "# Printing the evaluation metrics for 'Benign' class\n",
        "print(\"Accuracy (Benign):\", accuracy_benign)\n",
        "print(\"F1 Score (Benign):\", f1_benign)\n",
        "print(\"Recall (Benign):\", recall_benign)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHc7-8f9R9Sy",
        "outputId": "ceaa94e4-c5de-403d-fee5-08ef6c4c0b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (Cerber): 0.9837837837837838\n",
            "F1 Score (Cerber): 0.9918256130790191\n",
            "Recall (Cerber): 0.9837837837837838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(\"/content/Dataset.txt\", header=None)\n",
        "\n",
        "# Split the dataset into features (x) and target (y)\n",
        "x = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Resample the dataset to handle class imbalance\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "x_resampled, y_resampled = oversampler.fit_resample(x, y)\n",
        "\n",
        "# Split the resampled data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=75)\n",
        "\n",
        "# Instantiate and train the RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=50, random_state=10)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Filter predictions and true labels for 'Cerber' class\n",
        "cerber_indices = (y_test == 'Cerber')\n",
        "y_test_cerber = y_test[cerber_indices]\n",
        "y_pred_cerber = y_pred[cerber_indices]\n",
        "\n",
        "# Calculate evaluation metrics for 'Cerber' class\n",
        "accuracy_cerber = accuracy_score(y_test_cerber, y_pred_cerber)\n",
        "f1_cerber = f1_score(y_test_cerber, y_pred_cerber, average='weighted')\n",
        "recall_cerber = recall_score(y_test_cerber, y_pred_cerber, average='weighted')\n",
        "\n",
        "# Printing the evaluation metrics for 'Cerber' class\n",
        "print(\"Accuracy (Cerber):\", accuracy_cerber)\n",
        "print(\"F1 Score (Cerber):\", f1_cerber)\n",
        "print(\"Recall (Cerber):\", recall_cerber)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64-5bXwScuGX",
        "outputId": "c24ec962-46d1-496b-adad-4d3d7a7186f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Reventon): 0.9270650263620387\n",
            "F1 Score (Reventon): 0.9271474449722771\n",
            "Recall (Reventon): 0.9270650263620387\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(\"/content/Dataset.txt\", header=None)\n",
        "\n",
        "# Split the dataset into features (x) and target (y)\n",
        "x = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Resample the dataset to handle class imbalance\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "x_resampled, y_resampled = oversampler.fit_resample(x, y)\n",
        "\n",
        "# Split the resampled data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=75)\n",
        "\n",
        "# Instantiate and train the RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=50, random_state=10)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics for 'Reventon' class\n",
        "accuracy_reventon = accuracy_score(y_test, y_pred)\n",
        "f1_reventon = f1_score(y_test, y_pred, average='weighted')\n",
        "recall_reventon = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Printing the evaluation metrics for 'Reventon' class\n",
        "print(\"Accuracy (Reventon):\", accuracy_reventon)\n",
        "print(\"F1 Score (Reventon):\", f1_reventon)\n",
        "print(\"Recall (Reventon):\", recall_reventon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEkBtyebdA9_",
        "outputId": "713eb8bd-a646-4cbc-efbc-04cd03ee0f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy(Locky): 0.9270650263620387\n",
            "Precision (Locky): 1.0\n",
            "F1 Score (Locky): 0.9714285714285714\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(\"/content/Dataset.txt\", header=None)\n",
        "\n",
        "# Split the dataset into features (x) and target (y)\n",
        "x = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Resample the dataset to handle class imbalance\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "x_resampled, y_resampled = oversampler.fit_resample(x, y)\n",
        "\n",
        "# Split the resampled data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=75)\n",
        "\n",
        "# Instantiate and train the RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=50, random_state=10)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Calculate accuracy for 'Locky' class\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy(Locky):\", accuracy)\n",
        "\n",
        "# Filter predictions and true labels for 'Locky' class\n",
        "locky_indices = (y_test == 'Locky')\n",
        "y_test_locky = y_test[locky_indices]\n",
        "y_pred_locky = y_pred[locky_indices]\n",
        "\n",
        "# Calculate recall for each class\n",
        "recall_per_class = recall_score(y_test_locky, y_pred_locky, average=None)\n",
        "\n",
        "# Find the index corresponding to the 'Locky' class\n",
        "locky_index = classifier.classes_.tolist().index('Locky')\n",
        "\n",
        "# Calculate precision for 'Locky' class\n",
        "precision_per_class = precision_score(y_test_locky, y_pred_locky, average=None)\n",
        "\n",
        "# Extract precision value for the 'Locky' class\n",
        "precision_locky = precision_per_class[locky_index]\n",
        "\n",
        "print(\"Precision (Locky):\", precision_locky)\n",
        "\n",
        "# Calculate F1-score for 'Locky' class\n",
        "f1_per_class = f1_score(y_test_locky, y_pred_locky, average=None)\n",
        "\n",
        "# Extract F1-score value for the 'Locky' class\n",
        "f1_locky = f1_per_class[locky_index]\n",
        "\n",
        "print(\"F1 Score (Locky):\", f1_locky)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUVUQ69PqNHU",
        "outputId": "f7359daa-8043-4b10-b97c-7ccd7dec99b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (teslacrypt): 0.9545454545454546\n",
            "F1 Score (teslacrypt): 0.9767441860465117\n",
            "Recall (teslacrypt): 0.9545454545454546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "# Your code for calculating evaluation metrics\n",
        "\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(\"/content/Dataset.txt\", header=None)\n",
        "\n",
        "# Split the dataset into features (x) and target (y)\n",
        "x = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Resample the dataset to handle class imbalance\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "x_resampled, y_resampled = oversampler.fit_resample(x, y)\n",
        "\n",
        "# Split the resampled data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=75)\n",
        "\n",
        "# Instantiate and train the RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=50, random_state=10)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Filter predictions and true labels for 'Benign' class\n",
        "teslacrypt_indices = (y_test == 'teslacrypt')\n",
        "y_test_teslacrypt = y_test[teslacrypt_indices]\n",
        "y_pred_teslacrypt = y_pred[teslacrypt_indices]\n",
        "\n",
        "# Calculate evaluation metrics for 'Benign' class\n",
        "accuracy_teslacrypt = accuracy_score(y_test_teslacrypt, y_pred_teslacrypt)\n",
        "f1_teslacrypt = f1_score(y_test_teslacrypt, y_pred_teslacrypt, average='weighted')\n",
        "recall_teslacrypt = recall_score(y_test_teslacrypt, y_pred_teslacrypt, average='weighted')\n",
        "\n",
        "# Printing the evaluation metrics for 'Benign' class\n",
        "print(\"Accuracy (teslacrypt):\", accuracy_teslacrypt)\n",
        "print(\"F1 Score (teslacrypt):\", f1_teslacrypt)\n",
        "print(\"Recall (teslacrypt):\", recall_teslacrypt)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(\"/content/Dataset.txt\", header=None)\n",
        "\n",
        "# Split the dataset into features (x) and target (y)\n",
        "x = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Resample the dataset to handle class imbalance\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "x_resampled, y_resampled = oversampler.fit_resample(x, y)\n",
        "\n",
        "# Split the resampled data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=75)\n",
        "\n",
        "# Instantiate and train the RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=50, random_state=10)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Filter predictions and true labels for 'Yakes' class\n",
        "yakes_indices = (y_test == 'Yakes')\n",
        "y_test_yakes = y_test[yakes_indices]\n",
        "y_pred_yakes = y_pred[yakes_indices]\n",
        "\n",
        "# Calculate evaluation metrics for 'Yakes' class\n",
        "accuracy_yakes = accuracy_score(y_test_yakes, y_pred_yakes)\n",
        "f1_yakes = f1_score(y_test_yakes, y_pred_yakes, average='weighted')\n",
        "recall_yakes = recall_score(y_test_yakes, y_pred_yakes, average='weighted')\n",
        "\n",
        "# Printing the evaluation metrics for 'Yakes' class\n",
        "print(\"Accuracy (Yakes):\", accuracy_yakes)\n",
        "print(\"F1 Score (Yakes):\", f1_yakes)\n",
        "print(\"Recall (Yakes):\", recall_yakes)\n"
      ],
      "metadata": {
        "id": "VpP-QQ5_oxA5",
        "outputId": "6441dc47-e843-48a3-b103-6b258327c65a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Yakes): 0.8609625668449198\n",
            "F1 Score (Yakes): 0.9252873563218391\n",
            "Recall (Yakes): 0.8609625668449198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}